* 项目地址
计划借鉴的项目：
https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch
[[
https://github.com/ThilinaRajapakse/BERT_binary_text_classification]]
[[https://github.com/Tencent/NeuralNLP-NeuralClassifier]]
[[https://www.zhihu.com/question/19929473/answer/90201148]]
[[https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification][huggingface官方示例]]


* torch
核心对象：
tensor视作多维数组，不像矩阵，相乘是对应元素分别乘:一阶tensor是向量，二阶tensor是矩阵
requires_grad是tensor的一个属性，属性为true的话可以求关于这个tensor的导数，就是说这个tensor可以算自变量，见[[https://zhuanlan.zhihu.com/p/279758736]]
.backward()执行后会对得到当前tensor的所有相关项求梯度，最后储存在对应项的.grad属性里
.autograd.grad(input,output)求output关于input的偏导
* torch.nn:
包含神经网路操作方面的类
nn.Module


* 模型的一般架构:
准备数据：通过 DataLoader 加载数据。
定义损失函数和优化器。
前向传播：计算模型的输出。
计算损失：与目标进行比较，得到损失值。
反向传播：通过 loss.backward() 计算梯度。
更新参数：通过 optimizer.step() 更新模型的参数。
重复上述步骤，直到达到预定的训练轮数


* 训练的准备
使用神经网络时，核心需要提供：  
1. 模型结构（定义网络层）  
2. 优化器（如Adam、SGD）  
3. 损失函数（如CrossEntropyLoss）  
4. 训练数据（包括预处理和数据加载）  
5. 训练循环（实现完整的训练流程）  
6. 超参数（控制训练过程）  
7. 评估指标（验证模型性能）  
8. 模型保存/加载（持久化模型）  

1. 模型架构定义
定义神经网络的结构，包括层数、每层神经元数量、激活函数等。  
示例：  
#+BEGIN_SRC python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MyModel, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        return x
#+END_SRC


2. 优化器（Optimizer）
指定参数更新的算法，如SGD、Adam、RMSprop等，并设置学习率等超参数。  
示例：  
#+BEGIN_SRC python
import torch.optim as optim

model = MyModel(input_size=10, hidden_size=20, output_size=2)
optimizer = optim.Adam(model.parameters(), lr=0.001)  # 您提到的optimizer
#+END_SRC


3. 损失函数（Loss Function）
定义模型预测与真实标签之间的差异度量，如交叉熵、均方误差等。  
示例：  
#+BEGIN_SRC python
criterion = nn.CrossEntropyLoss()  # 分类任务常用交叉熵
#+END_SRC


4. 训练数据和数据处理
提供训练数据，并进行预处理（如归一化、增强等）。  
示例：  
#+BEGIN_SRC python
#+BEGIN_SRC python
from torch.utils.data import DataLoader, TensorDataset

# 假设已有训练数据
X_train = torch.randn(100, 10)  # 100个样本，每个样本10维特征
y_train = torch.randint(0, 2, (100,))  # 二分类标签

# 创建数据加载器
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
#+END_SRC


5. 训练循环
实现完整的训练逻辑，包括前向传播、计算损失、反向传播和参数更新。  
示例：  
#+BEGIN_SRC python
num_epochs = 10
for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        # 前向传播
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # 反向传播和优化
        optimizer.zero_grad()  # 梯度清零
        loss.backward()        # 计算梯度
        optimizer.step()       # 更新参数
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}")
#+END_SRC




6. 超参数设置
手动调整影响模型训练和性能的参数，如：
- 学习率（`learning_rate`）
- 批次大小（`batch_size`）
- 训练轮数（`epochs`）
- 正则化系数（如L2正则化的`weight_decay`）
- 优化器动量（如SGD的`momentum`）


7. 评估指标和验证逻辑
定义用于评估模型性能的指标（如准确率、F1分数、MSE等），并实现验证逻辑。  
示例：  

#+BEGIN_SRC python
from sklearn.metrics import accuracy_score

def evaluate(model, data_loader):
    model.eval()
    predictions = []
    true_labels = []
    with torch.no_grad():
        for inputs, labels in data_loader:
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            predictions.extend(preds.tolist())
            true_labels.extend(labels.tolist())
    return accuracy_score(true_labels, predictions)

# 在验证集上评估
val_accuracy = evaluate(model, val_loader)
print(f"Validation Accuracy: {val_accuracy:.4f}")
#+END_SRC


8. 模型保存与加载
实现模型的保存和加载逻辑，以便后续使用。  
示例：
#+BEGIN_SRC python
# 保存模型
torch.save(model.state_dict(), 'model.pth')

# 加载模型
loaded_model = MyModel(input_size=10, hidden_size=20, output_size=2)
loaded_model.load_state_dict(torch.load('model.pth'))
loaded_model.eval()
#+end_src
