* 使用torch处理数据
#+BEGIN_SRC python
  import torch
  torch.arange(12):tensor([0,...,11])
  x.shape:[12]
  x.numl:12
  x.sum() #可以指定坐标轴的方向
  x.reshape(3,4):tensor([[0,1,2,3],
                         [4,5,6,7],
                         [8,9,10,11]])
  torch.tensor([[1,2,3],[1,2,3]])#转换列表
  torch.zeros((2,3,4))
  torch.ones((2,3,4))
  torch.size(tensor)#返回一个数组
  #tensor间运算不视为矩阵，一一对应进行计算
  torch.exp(x)#x中每个元素分别进行自然指数运算
  x*y #x和y的元素一一对应相乘
  torch.cat((x,y),dim=0) #x和y从dim0上拼在一起
  #e.g. x.shape=[1,3,4],y.shape=[2,3,4]合成后为[3,3,4]
  x==y #返回一个tensor，分别是对应位置x和y是否相等
  x[1,2]=12#修改指定位置的元素
  x[0:2,1]=12#修改一系列元素
  #多用x+=y x[:]=x+y节约内存
  #torch.tensor()可以转换numpy中对象

#+END_SRC
* 使用pandas数据预处理
#+begin_src python
  import pandas as pd
  data=pd.read_csv(data_file)
  
#+end_src
* 线性代数需要知道的部分
#+begin_src python
  A.T #转置
  #向量是一维的tensor，矩阵是二维的tensor
  torch.dot(x,y)#点积
  torch.mv(A,x)
  torch.mv(A,B)#矩阵乘法和矩阵向量乘法
  torch.norm(u)#L2范数||u||,也适用于矩阵
  torch.abs(u)#L1范数

#+end_src
* 导数推广到向量
#+begin_src text
  x是(列)向量，y是标量
  dy/dx=[dy/x1,...,dy/xn]   (行向量)
  x是标量，y是(列)向量
  dy/dx=[dy1/x,...,dyn/x].T (列向量)
#+end_src
* 自动求导
计算一个函数在指定值上的导数；不同于符号求导和数值求导

步骤：
将代码分解成操作子,将计算表示成一棵树
e.g.
#+begin_src python
  c=2*a+b
  #是一棵如下的树：
  #c=(+ (* 2 a) b)
#+end_src
** 正向积累
做 e=2*a+c*b

计算机会先生成da/da db/da dc/da

再生成d(2*a)/da dc*b/db dc*b/dc

最后求de/da
** 反向传递
求 c=2*a+c*b 对a的导数时

先求 dc/d(2*a) dc/d(c*b)

再求 d2*a/da

反向传递是求时再算，可以减少对一些中间过程的储存；正向积累是边算边求导
** 实现
#+begin_src python
  x.requires_grad_(True)#要对x求导要先给x存放结果的标签
  y.backward()#这个函数是反向传递求导，会给一路上用到的所有tensor变量求导，把对应的结果储存到x.grad里
  x.grad#之后就可以在这里得到y对x求导的值  
#+end_src
